{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effdfcce-31f2-4ae1-b559-ab03b002a31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/marston/.local/lib/python3.12/site-packages (4.47.1)\n",
      "Requirement already satisfied: datasets in /home/marston/.local/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: huggingface_hub in /home/marston/.local/lib/python3.12/site-packages (0.24.6)\n",
      "Requirement already satisfied: torch in /home/marston/.local/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: evaluate in /home/marston/.local/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/marston/.local/lib/python3.12/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/marston/.local/lib/python3.12/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /home/marston/.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/marston/.local/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/marston/.local/lib/python3.12/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/marston/.local/lib/python3.12/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/marston/.local/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/marston/.local/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/marston/.local/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/marston/.local/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/marston/.local/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/marston/.local/lib/python3.12/site-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from torch) (68.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/marston/.local/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/marston/.local/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/marston/.local/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/marston/.local/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/marston/.local/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/marston/.local/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/marston/.local/lib/python3.12/site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/marston/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/marston/.local/lib/python3.12/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/marston/.local/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/marston/.local/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/marston/.local/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets huggingface_hub torch evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d187a-c57b-4239-bdc7-27c6d758dbca",
   "metadata": {},
   "source": [
    "Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98c8d42-2c79-4161-a9df-a0a08c664f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read CSV with utf-8 encoding.\n",
      "Warning: Long RAG response at index 25, length: 5020\n",
      "Warning: Long RAG response at index 35, length: 5113\n",
      "Warning: Long RAG response at index 61, length: 5266\n",
      "Warning: Long RAG response at index 68, length: 5134\n",
      "Warning: Long RAG response at index 97, length: 5019\n",
      "Metrics calculated and saved to 'metrics_results.csv'.\n",
      "Low score records saved to 'low_score_records.csv'.\n",
      "\n",
      "Average Metrics:\n",
      "Contextual Precision    0.686620\n",
      "Contextual Recall       0.725277\n",
      "Contextual Relevancy    0.664052\n",
      "Answer Relevancy        0.637604\n",
      "Faithfulness            0.524522\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Initialize Sentence-BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2').to('cuda' if torch.cuda.is_available() else 'cpu')  # Use CPU\n",
    "# Initialize stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define metric calculation functions\n",
    "def compute_contextual_precision(answer, rag_response, threshold=0.4):\n",
    "    answer_sentences = nltk.sent_tokenize(answer)\n",
    "    rag_sentences = nltk.sent_tokenize(rag_response)\n",
    "\n",
    "    if not rag_sentences or not answer_sentences:\n",
    "        return 0.0\n",
    "    \n",
    "    answer_embeddings = model.encode(answer_sentences, convert_to_tensor=True)\n",
    "    rag_embeddings = model.encode(rag_sentences, convert_to_tensor=True)\n",
    "    \n",
    "    relevant_count = 0\n",
    "    for rag_emb in rag_embeddings:\n",
    "        similarities = util.cos_sim(rag_emb, answer_embeddings)\n",
    "        if similarities.max().item() > threshold:\n",
    "            relevant_count += 1\n",
    "    \n",
    "    return relevant_count / len(rag_sentences)\n",
    "\n",
    "def compute_contextual_recall(answer, rag_response, threshold=0.4):\n",
    "    # Tokenize the answer and the RAG response into sentences\n",
    "    answer_sentences = nltk.sent_tokenize(answer)\n",
    "    rag_sentences = nltk.sent_tokenize(rag_response)\n",
    "\n",
    "    if not answer_sentences or not rag_sentences:\n",
    "        return 0.0\n",
    "        \n",
    "    # Encode the sentences into embeddings using the pre-trained model\n",
    "    answer_embeddings = model.encode(answer_sentences, convert_to_tensor=True)\n",
    "    rag_embeddings = model.encode(rag_sentences, convert_to_tensor=True)\n",
    "    \n",
    "    covered_count = 0\n",
    "    for answer_emb in answer_embeddings:\n",
    "        similarities = util.cos_sim(answer_emb, rag_embeddings)\n",
    "        if similarities.max() > threshold:\n",
    "            covered_count += 1\n",
    "    \n",
    "    return covered_count / len(answer_sentences)\n",
    "\n",
    "# compute contextual relevancy\n",
    "def compute_contextual_relevancy(question, answer, rag_response):\n",
    "    combined_ref = question + \" \" + answer\n",
    "    ref_embedding = model.encode(combined_ref, convert_to_tensor=True)\n",
    "    rag_embedding = model.encode(rag_response, convert_to_tensor=True)\n",
    "\n",
    "    similarity = util.cos_sim(ref_embedding, rag_embedding).item()\n",
    "    return similarity\n",
    "\n",
    "# compute answer relevancy\n",
    "def compute_answer_relevancy(question, rag_response):\n",
    "    question_embedding = model.encode(question, convert_to_tensor=True)\n",
    "    rag_embedding = model.encode(rag_response, convert_to_tensor=True)\n",
    "\n",
    "    similarity = util.cos_sim(question_embedding, rag_embedding).item()\n",
    "    return similarity\n",
    "\n",
    "def compute_faithfulness(answer, rag_response):\n",
    "    try:\n",
    "        # Semantic similarity\n",
    "        answer_embedding = model.encode(answer, convert_to_tensor=True)\n",
    "        rag_embedding = model.encode(rag_response, convert_to_tensor=True)\n",
    "        semantic_similarity = util.cos_sim(answer_embedding, rag_embedding).item()\n",
    "        \n",
    "        # Keyword overlap\n",
    "        answer_tokens = set(word_tokenize(answer.lower())) - stop_words\n",
    "        rag_tokens = set(word_tokenize(rag_response.lower())) - stop_words\n",
    "        if not answer_tokens or not rag_tokens:\n",
    "            keyword_overlap = 0.0\n",
    "        else:\n",
    "            common_tokens = answer_tokens.intersection(rag_tokens)\n",
    "            keyword_overlap = len(common_tokens) / len(answer_tokens)\n",
    "        \n",
    "        # Combine scores (weighted average, adjustable)\n",
    "        faithfulness_score = (semantic_similarity + keyword_overlap) / 2\n",
    "        return faithfulness_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_faithfulness: {e}\")\n",
    "        with open('faithfulness_errors.log', 'a') as f:\n",
    "            f.write(f\"Error: {e}\\nrag_response: {rag_response[:500]}...\\n\\n\")\n",
    "        return 0.0\n",
    "\n",
    "def plot_metric_distributions(results_df):\n",
    "    metrics = ['Contextual Precision', 'Contextual Recall', 'Contextual Relevancy', \n",
    "               'Answer Relevancy', 'Faithfulness']\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, metric in enumerate(metrics, 1):\n",
    "        plt.subplot(2, 3, i)\n",
    "        plt.hist(results_df[metric], bins=20, edgecolor='black')\n",
    "        plt.title(metric)\n",
    "        plt.xlabel('Score')\n",
    "        plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metric_distributions.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "    df = None\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv('Result_prompt/medqa3_dataset_rag_prompt.csv', encoding=encoding)\n",
    "            print(f\"Successfully read CSV with {encoding} encoding.\")\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to read CSV with {encoding} encoding. Trying next encoding...\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: CSV file not found.\")\n",
    "            return\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"Error: Could not read CSV with any supported encoding.\")\n",
    "        return\n",
    "    \n",
    "    required_columns = ['Question', 'Answer', 'RAG Response']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(\"Error: CSV must contain 'Question', 'Answer', 'RAG Response' columns.\")\n",
    "        return\n",
    "    \n",
    "    results = []\n",
    "    low_score_records = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        question = str(row['Question'])\n",
    "        answer = str(row['Answer'])\n",
    "        rag_response = str(row['RAG Response'])\n",
    "        \n",
    "        # Log long RAG responses for debugging\n",
    "        if len(rag_response) > 5000:\n",
    "            print(f\"Warning: Long RAG response at index {idx}, length: {len(rag_response)}\")\n",
    "        \n",
    "        metrics = {\n",
    "            'Contextual Precision': compute_contextual_precision(answer, rag_response),\n",
    "            'Contextual Recall': compute_contextual_recall(answer, rag_response),\n",
    "            'Contextual Relevancy': compute_contextual_relevancy(question, answer, rag_response),\n",
    "            'Answer Relevancy': compute_answer_relevancy(question, rag_response),\n",
    "            'Faithfulness': compute_faithfulness(answer, rag_response)\n",
    "        }\n",
    "        \n",
    "        if metrics['Contextual Precision'] < 0.1:\n",
    "            low_score_records.append(metrics)\n",
    "        \n",
    "        results.append(metrics)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('metrics_results.csv', index=False)\n",
    "    print(\"Metrics calculated and saved to 'metrics_results.csv'.\")\n",
    "    \n",
    "    # Save low score records for inspection\n",
    "    if low_score_records:\n",
    "        low_score_df = pd.DataFrame(low_score_records)\n",
    "        low_score_df.to_csv('low_score_records.csv', index=False)\n",
    "        print(f\"Low score records saved to 'low_score_records.csv'.\")\n",
    "    \n",
    "    plot_metric_distributions(results_df)\n",
    "    \n",
    "    avg_metrics = results_df[['Contextual Precision', 'Contextual Recall', \n",
    "                             'Contextual Relevancy', 'Answer Relevancy', \n",
    "                             'Faithfulness']].mean()\n",
    "    print(\"\\nAverage Metrics:\")\n",
    "    print(avg_metrics)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
